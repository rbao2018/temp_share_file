**slime_install_669343.sh**
```bash
# Copyright 2025 rbao2018. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -x

sed -i 's/export NCCL_DEBUG="INFO"/export NCCL_DEBUG="WARN"/g' /etc/profile.d/pouchenv.sh

sudo yum install numactl-libs numactl-devel -y

uv pip install jsonlines nvitop sglang-router --force-reinstall -i https://pypi.antfin-inc.com/simple
uv pip install "tensorboard==2.16.2" -i https://pypi.antfin-inc.com/simple

# reinstall torch_memory_saver
pip uninstall torch_memory_saver -y
CXXFLAGS="-std=c++17" pip install "git+https://github.com/fzyzcjy/torch_memory_saver.git"

# reinstall slime
pip uninstall slime -y && rm -rf /root/slime
cd /root && git clone -b "250901" https://github.com/rbao2018/slime.git && pip install /root/slime

# reinstall megatron-core with patch
pip uninstall megatron-core -y && rm -rf /root/Megatron-LM
cd /root && git clone -b "core_v0.13.1" https://github.com/NVIDIA/Megatron-LM.git
cp /root/slime/docker/patch/v0.4.10-cu126/megatron.patch /root/Megatron-LM/
cd /root/Megatron-LM && git apply --3way megatron.patch
pip install /root/Megatron-LM -i https://pypi.antfin-inc.com/simple


# reinstall sglang with patch
pip uninstall sglang -y && rm -rf /root/sglang
cd /root && git clone https://github.com/sgl-project/sglang.git
cd /root/sglang && git fetch --all && git checkout 5c14515feca116ff31c665484d01fd416597341b
yes | cp -rf /root/slime/rbao2018/install/pyproject.toml /root/sglang/python/
cp /root/slime/docker/patch/v0.4.10-cu126/sglang.patch /root/sglang
cd /root/sglang && git apply --3way sglang.patch
# Check for conflicts
if grep -R -n '^<<<<<<< ' .; then
    echo "Patch failed to apply cleanly. Please resolve conflicts."
    exit 1
fi
rm -rf sglang.patch

uv pip install "/root/sglang/python[all]" -i https://pypi.antfin-inc.com/simple


# export PYTHONPATH="/root/Megatron-LM:$PYTHONPATH"
# python -c "from megatron.core.optimizer_param_scheduler import OptimizerParamScheduler"
# python -c "from megatron.training.checkpointing import get_checkpoint_name, get_checkpoint_tracker_filename, save_checkpoint"
# python -c "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func"
# python -c "import apex;import torch;import fused_weight_gradient_mlp_cuda"
# python -c "import torch; print(torch.backends.cudnn.version())"
# python -c "import sglang; print(sglang.__version__)"


cd /root && git clone https://github.com/deepseek-ai/DeepEP.git
cd /root/DeepEP && MAX_JOBS=32 python setup.py install

# start math-verify server in advance
bash /workspace/bin/prepare_infra/evaluation/serve_math_verify/serve_hf_math_verify.sh
```


**slime_install_from_scratch.sh**
```bash

# Copyright 2025 rbao2018. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -x

# FROM reg.docker.alibaba-inc.com/aii/aistudio:aistudio-195898721-3620778887-1755620561162

pip uninstall deepspeed trl verl sglang unsloth unsloth-zoo flashinfer atorch atorch-addon flash_attn flash_infer ray mdatasets bitsandbytes -y

pip install uv -i https://pypi.antfin-inc.com/simple
uv pip install "flashinfer-python==0.2.9.rc2" -i https://pypi.antfin-inc.com/simple
uv pip install "sglang[all]==0.4.10.post2" -i https://pypi.antfin-inc.com/simple
uv pip install "ray[default,adag,cgraph]" "httpx[http2]" wandb pylatexenc blobfile accelerate "mcp[cli]" -i https://pypi.antfin-inc.com/simple

# cumem_allocator
cd /root && git clone https://github.com/zhuzilin/cumem_allocator.git && pip install -e /root/cumem_allocator -i https://pypi.antfin-inc.com/simple

# install repo for antgroup internal use
uv pip install pandas codetiming hydra-core pylatexenc qwen-vl-utils wandb dill pybind11 math-verify mathruler -i https://pypi.antfin-inc.com/simple
uv pip install mbridge blobfile pebble word2number pytest py-spy pyext pre-commit ruff bitsandbytes -i https://pypi.antfin-inc.com/simple

# download slime
cd /root && git clone https://github.com/rbao2018/slime.git && pip install -e /root/slime --no-deps

# reinstall transformer engine
pip uninstall transformer_engine_cu12 transformer_engine_torch -y
# MAX_JOBS=64 NVTE_FRAMEWORK=pytorch pip install --no-build-isolation git+https://github.com/NVIDIA/TransformerEngine.git@stable
MAX_JOBS=64 pip install --no-build-isolation "transformer_engine[pytorch]>=2.5.0" -i https://pypi.antfin-inc.com/simple

# reinstall apex
pip uninstall apex -y && cd /root && git clone https://github.com/NVIDIA/apex.git && cd apex
NVCC_APPEND_FLAGS="--threads 4" pip -v install --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings "--build-option=--cpp_ext --cuda_ext --parallel 8" ./

# # reinstall flash attention 2
# pip uninstall flash-attn -y
# MAX_JOBS=64 pip install --no-build-isolation flash-attn==2.7.4.post1 -i https://pypi.antfin-inc.com/simple


# export PYTHONPATH="/root/Megatron-LM:$PYTHONPATH"
# python -c "from megatron.core.optimizer_param_scheduler import OptimizerParamScheduler"
# python -c "from megatron.training.checkpointing import get_checkpoint_name, get_checkpoint_tracker_filename, save_checkpoint"
# python -c "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func"
# python -c "import apex;import torch;import fused_weight_gradient_mlp_cuda"
# python -c "import torch; print(torch.backends.cudnn.version())"


# tar -xf cudnn-linux-x86_64-...._cuda12-archive.tar.xz

# # 复制头文件
# sudo cp -P cudnn-linux-x86_64-9.12.0.46_cuda12-archive/include/cudnn*.h /usr/local/cuda-12.6.3/include/
# sudo cp -P cudnn-linux-x86_64-9.12.0.46_cuda12-archive/lib/libcudnn* /usr/local/cuda-12.6.3/lib64/
# sudo chmod a+r /usr/local/cuda-12.6.3/include/cudnn*.h /usr/local/cuda-12.6.3/lib64/libcudnn*

# add to .bashrc and delete .bashrc the mount /nas command
# export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}
# export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

```

**eval_qwen25_7b.sh**
```bash
# Copyright 2025 rbao2018. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


set -x

# 获取格式 YYYYMMDD_HHMMSS
time_str=$(date +"%Y%m%d_%H%M%S")

source /root/slime/scripts/models/qwen2.5-7B.sh

CKPT_ARGS=(
   --hf-checkpoint /input/models/Qwen2.5-7B-Instruct
   --ref-load /root/GLM-Z1-9B-0414_torch_dist
   --load /root/GLM-Z1-9B-0414_slime/
   --save /root/GLM-Z1-9B-0414_slime/
   --save-interval 20
)

ROLLOUT_ARGS=(
   --prompt-data /input/baorong.bao/datasets/AM-Thinking-v1-Distilled/math_all_prompt.jsonl
   --rollout-function-path slime_plugins.rbao2018.sglang_rollout_pebble.generate_rollout
   --dynamic-sampling-filter-path slime_plugins.rbao2018.dynamic_sampling_filters.more_than_half_correct
   --input-key prompt
   --label-key label
   --apply-chat-template
   --rollout-shuffle
   --rm-type dapo
   --num-rollout 3000
   --rollout-batch-size 1024
   --n-samples-per-prompt 8
   --rollout-max-response-len 8192
   --rollout-temperature 1.0
   --global-batch-size 8192
   --balance-data
   --debug-rollout-only
   --save-debug-rollout-data "/input/baorong.bao/tmp/slime_outputs/${time_str}/qwen25_7B_passrate_half_data_{rollout_id}.pt"
)

EVAL_ARGS=(
   --eval-interval 20
   --eval-prompt-data aime /root/aime-2024/aime-2024.jsonl
   --n-samples-per-eval-prompt 16
   --eval-max-response-len 16384
   --eval-top-p 0.7
)

SGLANG_ARGS=(
   --rollout-num-gpus 16
   --rollout-num-gpus-per-engine 2
   --sglang-server-concurrency 32
   --sglang-disable-radix-cache
)

python3 /root/slime/train.py \
  --actor-num-nodes 2 \
  --actor-num-gpus-per-node 8 \
  --colocate \
  ${MODEL_ARGS[@]} \
  ${CKPT_ARGS[@]} \
  ${ROLLOUT_ARGS[@]} \
  ${OPTIMIZER_ARGS[@]} \
  ${GRPO_ARGS[@]} \
  ${DISTRIBUTED_ARGS[@]} \
  ${WANDB_ARGS[@]} \
  ${PERF_ARGS[@]} \
  ${SGLANG_ARGS[@]}
```

**train_dpsk_distill_7b.sh**
```bash
# Copyright 2025 rbao2018. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

set -x

echo "TENSORBOARD_DIR is "$TENSORBOARD_DIR
echo "TORCH_NCCL_AVOID_RECORD_STREAMS is "$TORCH_NCCL_AVOID_RECORD_STREAMS
echo "NCCL_ALGO is "$NCCL_ALGO
echo "NCCL_IB_CUDA_SUPPORT is "$NCCL_IB_CUDA_SUPPORT
echo "NCCL_NET_GDR_READ is" $NCCL_NET_GDR_READ
echo "VLLM_USE_V1 is "$VLLM_USE_V1
echo "NUM_OF_NODES is" $NUM_OF_NODES
echo "MOE_MLP_PREFIX is" $MOE_MLP_PREFIX
echo "CUDA_DEVICE_MAX_CONNECTIONS is" $CUDA_DEVICE_MAX_CONNECTIONS
echo "CUDA_LAUNCH_BLOCKING is" $CUDA_LAUNCH_BLOCKING
echo "NVTE_BATCH_MHA_P2P_COMM is" $NVTE_BATCH_MHA_P2P_COMM

# ------------------------------------- verl mcore sglang or vllm ---------------------------------- 


# 获取格式 YYYYMMDD_HHMMSS
time_str=$(date +"%Y%m%d_%H%M%S")
# 使用该时间字符串设置 AIS_MEMO（如果未设置）
if [[ -z "$AIS_MEMO" ]]; then
   AIS_CKPT_NAME="slime_save_${time_str}"
fi

echo "AIS_CKPT_NAME: $AIS_CKPT_NAME"

python /input/baorong.bao/prepare_infra/watchdog_ais_ckpt.py --model_path /input/baorong.bao/models/$AIS_CKPT_NAME >/root/watchdog_ais_ckpt.log 2>&1 &

# DeepSeek-R1-Distill-Qwen-7B
MODEL_ARGS=(
   --swiglu
   --num-layers 28
   --hidden-size 3584
   --ffn-hidden-size 18944
   --num-attention-heads 28
   --group-query-attention
   --num-query-groups 4
   --max-position-embeddings 131072
   --seq-length 4096
   --use-rotary-position-embeddings
   --disable-bias-linear
   --add-qkv-bias
   --normalization "RMSNorm"
   --norm-epsilon 1e-06
   --rotary-base 10000
   --vocab-size 152064
   --accumulate-allreduce-grads-in-fp32
   --attention-softmax-in-fp32
   --attention-backend flash
   --moe-token-dispatcher-type alltoall
   --untie-embeddings-and-output-weights
   --attention-dropout 0.0
   --hidden-dropout 0.0
)

CKPT_ARGS=(
   --hf-checkpoint /input/models/DeepSeek-R1-Distill-Qwen-7B
   --ref-load /input/models/DeepSeek-R1-Distill-Qwen-7B_mcore13_dcp
   --save-interval 100
   --save /input/baorong.bao/models/$AIS_CKPT_NAME
)

ROLLOUT_ARGS=(
   --rollout-function-path slime_plugins.rbao2018.sglang_rollout_pebble.generate_rollout
   --prompt-data /input/baorong.bao/datasets/zhuzilin/dapo-math-17k/dapo-math-17k.jsonl
   --dynamic-sampling-filter-path slime_plugins.rbao2018.dynamic_sampling_filters.is_reward_zero_std
   --input-key prompt
   --label-key label
   --apply-chat-template
   --num-rollout 3000
   --rollout-batch-size 256
   --rollout-num-process 800
   --rollout-max-response-len 16000
   --rollout-temperature 1.0
   --rollout-shuffle
   --n-samples-per-prompt 16
   --global-batch-size 1024
   --micro-batch-size 8
   --use-dynamic-batch-size
   --max-tokens-per-gpu 9000
   --balance-data
)


DISTRIBUTED_ARGS=(
   --tensor-model-parallel-size 2
   --pipeline-model-parallel-size 1
   --context-parallel-size 2
   --sequence-parallel
)

PERF_ARGS=(
   --recompute-granularity full
   --recompute-method uniform
   --recompute-num-layers 1
)

GRPO_ARGS=(
   --advantage-estimator grpo
   --use-kl-loss
   --kl-loss-coef 0.00
   --kl-loss-type low_var_kl
   --entropy-coef 0.00
   --use-tis
)

OPTIMIZER_ARGS=(
   --lr 1e-6
   --lr-decay-style constant
   --weight-decay 0.1
   --adam-beta1 0.9
   --adam-beta2 0.98
)

WANDB_ARGS=(
   --use-wandb
   --wandb-mode offline
   --wandb-group rbao2018_test
   --wandb-dir /input/baorong.bao/slime/outputs/
   --tensorboard-dir "/home/admin/logs/tfevent"
)


SGLANG_ARGS=(
   --rollout-num-gpus 8
   --rollout-num-gpus-per-engine 2
   --sglang-server-concurrency 32
   --sglang-max-running-requests 64
   --sglang-disable-radix-cache
)


python /root/slime/train_async.py \
   --actor-num-nodes 1 \
   --actor-num-gpus-per-node 8 \
   ${SGLANG_ARGS[@]} \
   ${MODEL_ARGS[@]} \
   ${CKPT_ARGS[@]} \
   ${ROLLOUT_ARGS[@]} \
   ${OPTIMIZER_ARGS[@]} \
   ${GRPO_ARGS[@]} \
   ${DISTRIBUTED_ARGS[@]} \
   ${WANDB_ARGS[@]} \
   ${PERF_ARGS[@]} \
   --log-passrate
   
```

